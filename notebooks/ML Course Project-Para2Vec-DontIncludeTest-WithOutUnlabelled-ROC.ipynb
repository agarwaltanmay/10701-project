{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Course Project\n",
    "## Identifying User Stance On Social Media via Semi-Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "#### Midsem Pipeline - \n",
    "\n",
    " - **Read Data**: Read text files to load all the words. \n",
    " - **Clean Data**: Remove stop-words, everything lowercase, dehashify hashtags.\n",
    " - **Format Data**: Create data in a format required by each baseline method. \n",
    " - **Baseline Approaches**: LSA, pLSA, Para2Vec, LDA Topic Modelling. The goal of these approaches is to create a \"FIXED SIZE\" and \"HIGH LEVEL\" feature representation for variable length tweets. These representations leverage our unlabelled data. \n",
    " - **Training**: Some Supervised Learning on the learned representation using the given labels. \n",
    " - **Evaluation**: Compare the different methods mentioned above on different datasets. \n",
    "\n",
    "#### Endsem Approaches - \n",
    " - LDA2Vec - https://www.datacamp.com/community/tutorials/lda2vec-topic-model\n",
    " - Gaussian LDA - https://rajarshd.github.io/papers/acl2015.pdf\n",
    " - Word Embeddings Informed Topic Models - http://proceedings.mlr.press/v77/zhao17a/zhao17a.pdf\n",
    " \n",
    "#### Reference\n",
    " - https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_LABELLED_DATA_TRUMP = \"../semeval2016-task6-domaincorpus/data-all-annotations/testdata-taskB-all-annotations.txt\"\n",
    "PATH_UNLABELLED_DATA_TRUMP = \"./../semeval2016-task6-domaincorpus/downloaded_Donald_Trump.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP\n",
    "# # Run in python console\n",
    "# import nltk; nltk.download('stopwords')\n",
    "\n",
    "# # Run in terminal or command prompt\n",
    "# !python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim\n",
    "# !pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models import LsiModel\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['via'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Sem-Eval Task A Data (Labelled)\n",
    "\n",
    "Interactive Visualization - http://www.saifmohammad.com/WebPages/StanceDataset.htm\n",
    "\n",
    "Targets - \n",
    " - Hilary Clinton\n",
    " - Atheism\n",
    " - Climate Change\n",
    " - Donald Trump\n",
    " - Feminism\n",
    " - Abortion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_pickle('X_train.pkl')\n",
    "X_test = pd.read_pickle('X_test.pkl')\n",
    "y_train = pd.read_pickle('y_train.pkl')\n",
    "y_test = pd.read_pickle('y_test.pkl')\n",
    "\n",
    "data_labelled_train = pd.concat([X_train, y_train], ignore_index=True, axis=1)\n",
    "data_labelled_train.columns = ['Tweet', 'Stance']\n",
    "data_labelled_test = pd.concat([X_test, y_test], ignore_index=True, axis=1)\n",
    "data_labelled_test.columns = ['Tweet', 'Stance']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(sent):\n",
    "    sent = str(sent)\n",
    "    \n",
    "    # Remove new line characters\n",
    "    sent = re.sub('\\s+', ' ', sent)\n",
    "\n",
    "    # Remove distracting single quotes\n",
    "    sent = re.sub(\"\\'\", \"\", sent)\n",
    "\n",
    "    # Remove distracting single quotes\n",
    "    sent = re.sub(\"\\\"\", \"\", sent)\n",
    "\n",
    "    # Remove hashtags\n",
    "    sent = re.sub(\"\\#\", \"\", sent)\n",
    "\n",
    "    # Remove http:// links\n",
    "    sent = re.sub('http:\\/\\/.*','', sent)\n",
    "\n",
    "    # Remove https:// links\n",
    "    sent = re.sub('https:\\/\\/.*','', sent)\n",
    "        \n",
    "    return sent\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_labelled_train.where(data_labelled_train.Stance == 'AGAINST').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labelled_train.where(data_labelled_train.Stance == 'NONE').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labelled_train.where(data_labelled_train.Stance == 'FAVOR').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labelled_test.where(data_labelled_test.Stance == 'AGAINST').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labelled_test.where(data_labelled_test.Stance == 'NONE').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labelled_test.where(data_labelled_test.Stance == 'FAVOR').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Sem-Eval Task B Data (Unlabelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = pd.concat([data_labelled_train, data_labelled_test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = combined_data['Tweet']\n",
    "# df = data_labelled['Tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.values.tolist()\n",
    "pprint(data[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentences To Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "pprint(data_words[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Bi-Grams Tri-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stop Words and Lemmatize\n",
    "\n",
    "WE NEED PRONOUNS FOR STANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV', 'PRON']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV', 'PRON'])\n",
    "\n",
    "print(data_lemmatized[:1]), print(len(data_lemmatized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "def read_corpus(texts, tokens_only=False):\n",
    "    for i, line in enumerate(texts):\n",
    "        if tokens_only:\n",
    "            yield line\n",
    "        else:\n",
    "            # For training data, add tags\n",
    "            yield gensim.models.doc2vec.TaggedDocument(line, [i])\n",
    "\n",
    "corpus = list(read_corpus(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=100, min_count=2, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = corpus[:627]\n",
    "model.build_vocab(train_corpus)\n",
    "print(model.corpus_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset for Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet_feats = []\n",
    "\n",
    "for i in range(len(corpus)):\n",
    "    representation = model.infer_vector(corpus[i].words)\n",
    "    mallet_feats.append(representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STANCES = ['AGAINST', 'FAVOR', 'NONE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_labels(label):\n",
    "    if label == 'AGAINST':\n",
    "        return 0\n",
    "    elif label == 'FAVOR':\n",
    "        return 1\n",
    "    elif label == 'NONE':\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_stance = combined_data.Stance\n",
    "labelled_stance = labelled_stance.apply(transform_labels)\n",
    "labelled_stance = labelled_stance.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mallet_feats[0]), len(mallet_feats), len(labelled_stance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "import argparse\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Train Test Split\n",
    "X_train = mallet_feats[:627]\n",
    "X_test = mallet_feats[627:]\n",
    "y_train = labelled_stance[:627]\n",
    "y_test = labelled_stance[627:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomForest(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    # GRID SEARCH\n",
    "    parameters = {'estimator__n_estimators':range(100,600,100), 'estimator__max_depth':range(1,20,5)}\n",
    "    \n",
    "    y_train = label_binarize(y_train, classes=[0, 1, 2])\n",
    "    y_test = label_binarize(y_test, classes=[0, 1, 2])\n",
    "    n_classes = 3\n",
    "    \n",
    "    rlf = OneVsRestClassifier(RandomForestClassifier(random_state=0))\n",
    "    rlf = GridSearchCV(rlf, parameters, cv=5)\n",
    "    rlf.fit(X_train, y_train)\n",
    "    \n",
    "    rf_cv = cross_val_score(rlf, X_train, y_train, cv=5, scoring='f1_macro')\n",
    "    report = {}\n",
    "    report[\"RF_cross_val_score\"] = rf_cv.tolist()\n",
    "    report[\"RF_mean_acc\"] = rf_cv.mean()\n",
    "    report[\"RF_std_acc\"] = rf_cv.std()*2\n",
    "    report[\"RF_best_estimator\"] = rlf.best_estimator_\n",
    "    \n",
    "    y_pred = rlf.predict(X_test)\n",
    "    \n",
    "    y_score = label_binarize(y_pred, classes=[0, 1, 2])\n",
    "    \n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro') \n",
    "    report[\"RF_F1_SCORE\"] = f1_macro\n",
    "    \n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = metrics.roc_curve(y_test[:, i], y_score[:, i])\n",
    "        roc_auc[i] = metrics.auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Plot all ROC curves\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "                 label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                 ''.format(i, roc_auc[i]))\n",
    "\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic to multi-class')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVMClassifier(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    parameters = {'estimator__kernel':['linear','rbf','poly'], 'estimator__C':[0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "     \n",
    "    y_train = label_binarize(y_train, classes=[0, 1, 2])\n",
    "    y_test = label_binarize(y_test, classes=[0, 1, 2])\n",
    "    n_classes = 3\n",
    "    \n",
    "    clf = OneVsRestClassifier(svm.SVC(gamma='auto'))\n",
    "    clf = GridSearchCV(clf, parameters, cv=5)\n",
    "    y_score = clf.fit(X_train, y_train).decision_function(X_test)\n",
    "    \n",
    "    clf_cv = cross_val_score(clf, X_train, y_train, cv=5, scoring='f1_macro')\n",
    "    report = {}\n",
    "    report[\"SVM_cross_val_score\"] = clf_cv.tolist()\n",
    "    report[\"SVM_mean_acc\"] = clf_cv.mean()\n",
    "    report[\"SVM_std_acc\"] = clf_cv.std()*2\n",
    "    report[\"SVM_best_estimator\"] = clf.best_estimator_\n",
    "\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro') \n",
    "    report[\"SVM_F1_SCORE\"] = f1_macro\n",
    "    \n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = metrics.roc_curve(y_test[:, i], y_score[:, i])\n",
    "        roc_auc[i] = metrics.auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Plot all ROC curves\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "                 label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                 ''.format(i, roc_auc[i]))\n",
    "\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic to multi-class')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    return report\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet_report = RandomForest(X_train, X_test, y_train, y_test)\n",
    "print(mallet_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet_report = SVMClassifier(X_train, X_test, y_train, y_test)\n",
    "print(mallet_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
